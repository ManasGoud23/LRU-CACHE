ğŸ“œ README: LRU Cache Implementation & Caching Concepts:

1ï¸âƒ£ What is Caching? Why is it Needed?
**Caching is a technique used to store frequently accessed data in a temporary storage space to improve retrieval speed. Instead of fetching data from a slow primary source (database, disk, or API) every time, caching helps reduce latency and improve system performance.

Need for Caching:
âœ” Faster Data Access â€“ Reduces load on primary storage.
âœ” Improves Performance â€“ Reduces response time in applications.
âœ” Efficient Memory Usage â€“ Stores only frequently used data.
âœ” Minimizes Redundant Computation â€“ Saves processing power.

2ï¸âƒ£ In-Memory Caching: Redis & Memcached
In-memory caching stores frequently accessed data in RAM instead of disks or databases, making retrieval significantly faster. Two popular in-memory caching systems are:

ğŸ”¹ Redis (Remote Dictionary Server)
âœ” Stores key-value pairs with optional data persistence.
âœ” Supports data structures like lists, sets, and hashes.
âœ” Used for real-time applications (leaderboards, chat systems).
âœ” Supports LRU eviction for efficient memory management.

ğŸ”¹ Memcached
âœ” High-speed distributed memory caching system.
âœ” Simple key-value storage model.
âœ” Multi-threaded and optimized for high-speed caching.
âœ” Used for reducing database load in web applications.

3ï¸âƒ£ Cache Memory in Computer Organization
Cache Memory is a high-speed memory located between the CPU and RAM, storing frequently used instructions for faster processing.

âœ” Levels of Cache Memory:

L1 Cache: Small, inside the CPU, fastest access.
L2 Cache: Slightly larger, located near the CPU.
L3 Cache: Shared among multiple CPU cores, larger but slower.
âœ” Cache Mapping Techniques:

Direct Mapped Cache â€“ Each block maps to a single cache line.
Fully Associative Cache â€“ Any block can be stored in any cache line.
Set-Associative Cache â€“ Hybrid approach allowing multiple placements.
4ï¸âƒ£ Different Cache Replacement Strategies
When the cache is full, the system must evict old data to store new data. Different strategies include:

ğŸ”¹ LRU (Least Recently Used) â€“ Removes least recently accessed item.
ğŸ”¹ FIFO (First In First Out) â€“ Removes oldest stored item.
ğŸ”¹ LFU (Least Frequently Used) â€“ Removes least frequently accessed data.
ğŸ”¹ Random Replacement â€“ Randomly selects an item for eviction.

5ï¸âƒ£ Designing LRU Cache using Doubly Linked List & HashMap
LRU Cache can be designed using a Doubly Linked List and HashMap:

âœ” Doubly Linked List â€“ Maintains the order of access (Most Recently Used â†’ Least Recently Used).
âœ” HashMap â€“ Provides O(1) access to stored key-value pairs.
âœ” Head & Tail Nodes â€“ Dummy nodes for easy insertion/deletion.

Operations:
âœ… GET (key): If key exists, move it to the front (Most Recently Used).
âœ… PUT (key, value): Add the new key to the front. If full, remove Least Recently Used (tail).

6ï¸âƒ£ UML Diagram for LRU Cache Design
plaintext
Copy
Edit

+----------------+
|   LRUCache     |
+----------------+
| - capacity     |
| - map (HashMap)|
| - head (Node)  |
| - tail (Node)  |
+----------------+
| + get(key)     |
| + put(key, val)|
| - insert(Node) |
| - delete(Node) |
+----------------+

           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  HashMap â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
    +----------------------------------+
    â”‚         Doubly Linked List       â”‚
    +----------------------------------+
    â”‚ head â‡„ Node1 â‡„ Node2 â‡„ ... â‡„ tailâ”‚
    +----------------------------------+
